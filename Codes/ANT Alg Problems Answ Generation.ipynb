{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d306d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code I used to prompt Qwen/Qwen2.5-Math-7B-Instruct to generate algorithms for algorithmic problems 1, ...,30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6c6e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "!pip install transformers huggingface_hub\n",
    "!pip install -q keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3f443c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "GoogleDrive= \"/content/drive/MyDrive\"\n",
    "\n",
    "Prompts = os.path.join(GoogleDrive, \"ANT\", \"Prompts Alg NT\")    #to use this code you must change this directory\n",
    "Save_Outputs = os.path.join(Prompts, \"outputs for problems\")    #to use this code you must change this directory\n",
    "\n",
    "os.makedirs(Save_Outputs)\n",
    "os.makedirs(\"./offload\")\n",
    "\n",
    "Hinting_Strategies = [\"Few-shot Hinting\", \"Chain-of-Thought (CoT) Prompting\", \"Automatic Chain-of-Thought (Auto-CoT) Prompting\", \"Self-Consistency\", \"Logical Chain-of-Thought (LogiCoT) Prompting\", \"Chain-of-Symbol (CoS) Prompting\", \"Structured Chain-of-Thought (SCoT) Prompting\", \"ReAct Prompting\", \"Clear and Specific Instructions\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aac4fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hugging_Face_token= \"...\" # put your hugging face token to be able to use this\n",
    "\n",
    "Model_ID   = \"Qwen/Qwen2.5-Math-7B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(Model_ID)\n",
    "tokenizer = AutoTokenizer.from_pretrained(Model_ID)\n",
    "pipe= pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869e4c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(1, 31):\n",
    "    docj= (Prompts / f\"Problem {j}.txt\").read_text(encoding=\"utf-8\")\n",
    "    question= re.search(r\"\\\\subsection\\*\\{Question\\}(.*?)\\\\subsection\\*\\{Hint(?:s)?\\}\", docj, re.DOTALL).group(1).strip()\n",
    "    Nine_Hints=  re.search(r\"\\\\subsection\\*\\{Hint(?:s)?\\}(.*?)(\\\\subsection\\*\\{Answer/Algorithm\\}|$)\", docj, re.DOTALL).group(1)\n",
    "    hints= re.findall(r\"\\\\subsection\\*\\{([^}]+)\\}(.*?)(?=\\\\subsection\\*\\{|$)\", Nine_Hints, re.DOTALL)\n",
    "    answers = {}\n",
    "    for i, (method, hint_i) in enumerate(hints, start=1):\n",
    "        prompt = (\n",
    "            f\"### Algorithmic Problem {j}\\n\\n\"\n",
    "            f\"**Question:** {question}\\n\\n\"\n",
    "            f\"**Hint ({method}):** {hint_i.strip()}\\n\\n\"\n",
    "            f\"**Generated Answer:**\"\n",
    "        )\n",
    "        output = pipe(prompt, max_new_tokens=1024, do_sample=False)[0][\"generated_text\"]\n",
    "        answer= output[len(prompt):].strip()\n",
    "        answers[f\"Hint_{i}\"] = {\n",
    "            \"method\": method,\n",
    "            \"prompt\": prompt,\n",
    "            \"generated_answer\": answer\n",
    "        }\n",
    "    outputs = os.path.join(Save_Outputs, f\"Problem_{j}_outputs.json\")\n",
    "    with open(outputs, \"w\", encoding=\"utf-8\") as f:json.dump(answers, f, indent=2)\n",
    "print(\"done\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
