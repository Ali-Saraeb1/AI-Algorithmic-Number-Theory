{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfee31f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "!pip install transformers huggingface_hub\n",
    "!pip install -q keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4873004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "GoogleDrive= \"/content/drive/MyDrive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c298c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hugging_Face_token= \"...\" # put your hugging face token to be able to use this\n",
    "MODEL_ID = \"MetaMath-7B-V1.0\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(Model_ID)\n",
    "tokenizer = AutoTokenizer.from_pretrained(Model_ID)\n",
    "pipe= pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634c80bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_answer(file_position: str) -> str:\n",
    "    with open(file_position, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    return content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176faae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Metrics = [\n",
    "    \"Final Accuracy\", \"Validity Rate\", \"Coherence Score\", \"Robustness Consistency\",\n",
    "    \"Calibration Confidence\", \"Correctness\", \"Efficiency\", \"Clarity\", \"Accuracy\",\n",
    "    \"Pass Rate\", \"Stepwise Correctness\", \"Redundancy\", \"Comp Efficiency\", \"Similarity\"\n",
    "]\n",
    "\n",
    "\n",
    "Metric_Definitions = {\n",
    "    \"Final Accuracy\":\n",
    "        \"Exact-match accuracy: fraction of test cases where the generated solution exactly matches the reference solution.\",\n",
    "    \"Validity Rate\":\n",
    "        \"Proportion of generated code answers that compile without syntax errors and satisfy basic functional tests.\",\n",
    "    \"Coherence Score\":\n",
    "        \"Entity-grid coherence: measures logical flow by tracking entity recurrence across sentences.\",\n",
    "    \"Robustness Consistency\":\n",
    "        \"Stability of outputs under small prompt perturbations, measured by change in evaluation metrics.\",\n",
    "    \"Calibration Confidence\":\n",
    "        \"Alignment between model-predicted probabilities and actual correctness, e.g. via expected calibration error.\",\n",
    "    \"Correctness\":\n",
    "        \"Fraction of intermediate and final reasoning steps that are mathematically or logically valid.\",\n",
    "    \"Efficiency\":\n",
    "        \"Resource usage of the presented solution, typically reported via time complexity or empirical runtime.\",\n",
    "    \"Clarity\":\n",
    "        \"Readability and organization of the explanation, e.g. via readability scores like Fleschâ€“Kincaid.\",\n",
    "    \"Accuracy\":\n",
    "        \"Standard classification accuracy: (TP + TN) / (TP + TN + FP + FN) over all test cases.\",\n",
    "    \"Pass Rate\":\n",
    "        \"pass@k metric: probability that at least one of the top-k generated samples passes all unit tests.\",\n",
    "    \"Stepwise Correctness\":\n",
    "        \"Proportion of individual chain-of-thought steps that are correct, as in REVEAL evaluations.\",\n",
    "    \"Redundancy\":\n",
    "        \"Degree to which the answer avoids unnecessary repetition or superfluous content.\",\n",
    "    \"Comp Efficiency\":\n",
    "        \"Computational efficiency of any algorithm presented, e.g. its Big-O time complexity.\",\n",
    "    \"Similarity\":\n",
    "        \"Semantic similarity (e.g. via BERTScore) between generated and reference answers.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a607c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hint_Labels = [f\"Hint {i}\" for i in range(1, 10)]\n",
    "\n",
    "def make_latex_table_template():\n",
    "    columns = \"|l|\" + \"c|\" * 9\n",
    "    header = \" & \".join(Hint_Labels)\n",
    "    rows = []\n",
    "    for m in Metrics:\n",
    "        rows.append(f\"{m:<26} & \" + \" & \".join([\"\" for _ in Hint_Labels]) + r\" \\\\\")\n",
    "    body = \"\\n\".join(rows)\n",
    "    return f\"\"\"\n",
    "\\\\begin{{tabular}}{{{columns}}}\n",
    "Metric                     & {header} \\\\\\\\ \\\\hline\n",
    "{body}\n",
    "\\\\end{{tabular}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d151941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(true_ans: str, generated: dict) -> str:\n",
    "    definitions = \"\\n\".join(f\"- **{m}**: {Metric_Definitions[m]}\" for m in Metrics)\n",
    "    table_tpl = make_latex_table_template()\n",
    "\n",
    "    prompt = (\n",
    "        \"You are given a reference answer and nine model-generated answers under different hints.\\n\"\n",
    "        \"Evaluate each generated answer *against* the reference on the following metrics, each as a real number in [0,1]:\\n\\n\"\n",
    "        f\"{definitions}\\n\\n\"\n",
    "        \"### Reference Answer:\\n\"\n",
    "        \"```\\n\"\n",
    "        f\"{true_ans}\\n\"\n",
    "        \"```\\n\\n\"\n",
    "        \"### Model-Generated Answers:\\n\"\n",
    "    )\n",
    "    for hint, info in generated.items():\n",
    "        prompt += f\"\\n#### {hint}\\n{info['generated_answer']}\\n\"\n",
    "\n",
    "    prompt += (\n",
    "        \"\\nNow, fill in the following LaTeX table with numeric scores in [0,1] \"\n",
    "        \"(no placeholders) for each hint and metric, and output *only* the LaTeX code:\\n\"\n",
    "        f\"{table_tpl}\"\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ca0e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(1, 31): #change these to use the code\n",
    "    true_path = f\"/content/drive/MyDrive/ANT/Prompts Alg NT/Question {j}.txt\"\n",
    "    gen_path  = f\"/content/drive/MyDrive/ANT/Prompts Alg NT/outputs for questions/Question_{j}_outputs.json\"\n",
    "    out_path  = f\"/content/drive/MyDrive/ANT/Prompts Alg NT/Outputs for Questions Evaluation/Question_{j}_evaluation.txt\"\n",
    "\n",
    "    true_answer0 = true_answer(true_path)\n",
    "    with open(gen_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        generated_answer = json.load(f)\n",
    "\n",
    "    prompt = build_prompt(true_answer0, generated_answer)\n",
    "    result = pipe(prompt, max_length=2048, temperature=0.0, top_p=0.9, do_sample=False, eos_token_id=tokenizer.eos_token_id)[0][\"generated_text\"]\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(result)\n",
    "\n",
    "    print(f\"Saved evaluation for Question {j} to {out_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
