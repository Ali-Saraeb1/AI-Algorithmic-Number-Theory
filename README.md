Cite here: https://zenodo.org/badge/DOI/10.5281/zenodo.15293187.svg

Recent benchmarks of large language models have mainly focused on general mathematics problems and the currently infeasible objective of automated theorem proving. In this project, we focus on a more specialized domain: we evaluate the performance of the state-of-the-art large language model *Qwen2.5-Math-7B-Instruct* on algorithmic and computational tasks in algorithmic number theory. 

On a benchmark of thirty algorithmic problems and thirty computational questions taken from classical number-theoretic textbooks and Math StackExchange, the model achieves at least 95% accuracy on every problem/question when given an optimal non-spoiling hint. Moreover, for a fixed hinting strategy, the mean accuracy peaks at 88% for algorithmic problems and 89% for computational questions, indicating the sensitivity of performance to the choice of hinting strategy. 

We refer to our constructed dataset as the *Hinted Algorithmic Number Theory* (HANT) dataset and make both the dataset and the accompanying code publicly available.
