Recent benchmarks of large language models have mainly focused on general mathematics problems and the currently infeasible objective of automated theorem-proving. In the first part this paper, we relax our ambition, and we focus on a more specialized domain: we evaluate the performance of the state-of-the-art large language model \textit{Qwen2.5‑Math‑7B‑Instruct} on algorithmic and computational tasks in algorithmic number theory. On a benchmark of thirty algorithmic problems and thirty computational questions taken from classical number-theoretic textbooks and Math StackExchange, the model achieves at least \(0.95\) accuracy on every problem/question when given an optimal non-spoiling hint. Moreover, for a fixed hinting strategy, the mean accuracy peaks at \(0.88\) for algorithmic problems and \(0.89\) for computational questions, indicating the sensitivity of performance to the choice of hinting strategy. We refer to our constructed dataset as the \emph{Hinted Algorithmic Number Theory} (HANT) dataset and make both the dataset and accompanying code publicly available 
