Cite:

Paper:Paper: @misc{saraeb2025artificial, title={Artificial Intelligence in Number Theory: LLMs for Algorithm Generation and Neural Networks for Conjecture Verification}, author={Ali Saraeb}, year={2025}, eprint={2504.19451}, archivePrefix={arXiv}, primaryClass={math.NT} }

Code: [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.15293187.svg)](https://doi.org/10.5281/zenodo.15293187)

Recent benchmarks of large language models have mainly focused on general mathematics problems and the currently infeasible objective of automated theorem proving. In this project, we focus on a more specialized domain: we evaluate the performance of the state-of-the-art large language model *Qwen2.5-Math-7B-Instruct* on algorithmic and computational tasks in algorithmic number theory. 

On a benchmark of thirty algorithmic problems and thirty computational questions taken from classical number-theoretic textbooks and Math StackExchange, the model achieves at least 95% accuracy on every problem/question when given an optimal non-spoiling hint. Moreover, for a fixed hinting strategy, the mean accuracy peaks at 88% for algorithmic problems and 89% for computational questions, indicating the sensitivity of performance to the choice of hinting strategy. 

We refer to our constructed dataset as the *Hinted Algorithmic Number Theory* (HANT) dataset and make both the dataset and the accompanying code publicly available.
